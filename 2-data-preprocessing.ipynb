{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Text Mining Group Project 23-24**\n",
    "\n",
    "##### **Group 3**\n",
    "\n",
    "**Elements**:\n",
    "- Afonso Gorj√£o | 20230575 | 20230575@novaims.unl.pt\n",
    "- Diogo Almeida | ... | ...@novaims.unl.pt\n",
    "- Frederico Portela | R20181072 | r20181072@novaims.unl.pt\n",
    "- Pedro Carvalho | 20230554 | 20230554@novaims.unl.pt\n",
    "\n",
    "**Index**\n",
    "1. [Library imports](#library_imports)\n",
    "2. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Library Imports**<a id='library_imports'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "# nltk.download('all')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import fasttext\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Constants**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "CORPORA_PATH = os.path.join('corpora', 'raw-data')\n",
    "\n",
    "TRAIN_INFO_PATH = os.path.join(CORPORA_PATH, 'train.xlsx')\n",
    "TRAIN_REVIEWS_PATH = os.path.join(CORPORA_PATH, 'train_reviews.xlsx')\n",
    "\n",
    "TEST_INFO_PATH = os.path.join(CORPORA_PATH, 'test.xlsx')\n",
    "TEST_REVIEWS_PATH = os.path.join(CORPORA_PATH, 'test_reviews.xlsx')\n",
    "\n",
    "TEST_MERGED_PATH = ''\n",
    "\n",
    "TRAIN_MERGED_TRAIN_PATH = ''\n",
    "TRAIN_MERGED_VAL_PATH = ''\n",
    "TRAIN_MERGED_TEST_PATH = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show we treat our reviews as independent or aggregate all reviews for each property?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to handle missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep from reviews the sentences pertaining to the property, discard stuff about their trip, e.g. the city and restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# due to sheer amount of data k-fold cross-validation will not be feasible\n",
    "# so a train-val-test split is preferred\n",
    "\n",
    "# TODO rewrite this code\n",
    "\n",
    "X = train_merged[[col for col in train_merged.columns if col != 'unlisted']]\n",
    "y = train_merged['unlisted']\n",
    "\n",
    "def two_step_proportions(train_p, val_p, test_p):\n",
    "    \"\"\"\n",
    "    Since we need to split the data in two steps\n",
    "    this function returns the proportions of the\n",
    "    'test_size' arg. needed to get the true prop.\n",
    "    \"\"\" \n",
    "    return (test_p, 1-(train_p/(1-test_p)))\n",
    "\n",
    "test_size_1, test_size_2 = two_step_proportions(.8, .1, .1)\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X,\n",
    "                                                            y,\n",
    "                                                            test_size=test_size_1,\n",
    "                                                            shuffle=True,\n",
    "                                                            stratify=y,\n",
    "                                                            random_state=SEED)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val,\n",
    "                                                  y_train_val,\n",
    "                                                  test_size=test_size_2,\n",
    "                                                  shuffle=True,\n",
    "                                                  stratify=y_train_val,\n",
    "                                                  random_state=SEED)\n",
    "\n",
    "get_prop = lambda d: round(len(d) / len(X) * 100, 2)\n",
    "print('Dataset shapes:')\n",
    "print(f'\\tTrain: {len(X_train)} ({get_prop(X_train)}%)')\n",
    "print(f'\\tVal: {len(X_val)} ({get_prop(X_val)}%)')\n",
    "print(f'\\tTest: {len(X_test)} ({get_prop(X_test)}%)')\n",
    "print(f\"\\nTotal observations: {len(X)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
